{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27969e9-ef82-4ae2-80a4-d209613e2ae6",
   "metadata": {},
   "source": [
    "## dataset handler (folder-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35418519-564e-41e3-9bfc-cd5fb72da88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import config as cfg\n",
    "\n",
    "from data_loader.dataset import dset_imageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe9242b-3081-4d4c-b3b2-90c7050a64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset from folders\n",
    "train_dataset, valid_dataset, test_dataset = dset_imageFolder()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=cfg.batch_size, shuffle=cfg.shuffle, num_workers=cfg.num_workers)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=cfg.batch_size, shuffle=cfg.shuffle, num_workers=cfg.num_workers)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=cfg.batch_size, shuffle=cfg.shuffle, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f27b92-a6ac-440c-9ae4-2a7252c320fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid dataset using train_dataset and random split\n",
    "# len_train = round(len(train_dataset) * 0.7)\n",
    "# len_valid = len(train_dataset) - len_train\n",
    "\n",
    "# train_dataset, valid_dataset = random_split(train_dataset, \n",
    "#                                             lengths=[len_train, len_valid], \n",
    "#                                             generator=torch.Generator().manual_seed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdea680b-6377-48c1-b08c-24ff3c8cb57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3435e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# demo for handler and loader\n",
    "samples = next(iter(train_loader))\n",
    "print(samples[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b01792-7d9c-47b2-8078-51f8e250c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep learning models : LeNet-5\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=75000, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f7588d-70d9-4750-9c23-df82229578f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Tanh()\n",
      "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=75000, out_features=84, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=84, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3202, 0.3324, 0.3473],\n",
       "        [0.3163, 0.3398, 0.3439]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet5(n_classes=len(train_dataset.dataset.classes))\n",
    "print(model)\n",
    "model(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817b9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (convolutions): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=75000, out_features=84, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=84, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device, cuda or cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimization algorithm\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c75537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d58bfcb3464842adc66ee56d45e235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# implement training process\n",
    "# use tqdm for progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "# epoch loop\n",
    "epochs = 5\n",
    "\n",
    "# training epoch\n",
    "for epoch in tqdm(range(0, epochs)):\n",
    "    \n",
    "    # training batch \n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):    \n",
    "\n",
    "        optimizer.zero_grad() # remove gradients from previous loop\n",
    "        \n",
    "        outputs = model(inputs) # classify inputs\n",
    "        \n",
    "        loss = loss_fn(outputs, targets) # calculate loss between outputs and targets\n",
    "        \n",
    "        loss.backward() # implement backpropagation\n",
    "        \n",
    "        optimizer.step() # update weights/parameters\n",
    "        \n",
    "        break\n",
    "    \n",
    "    # run validate model with validation data\n",
    "    # model.train(False)\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, targets) in enumerate(valid_loader):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9538ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4681, 0.3260, 0.2060], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e166e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement data/, loss/, models/, optimizer/, scheduler/, tools/ or utilities/\n",
    "\n",
    "# implement train.py\n",
    "\n",
    "# implement test.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ef86f732e41f4abf60c0089e3910b8223a8414c1f5207ae9efe89683b749b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
